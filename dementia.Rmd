---
title: "Dementia"
author: "Examination Number: Y3866839"
date: "30/11/2020"
output: 
  bookdown::html_document2: default
bibliography: references/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.retina = 3)

library(tidyverse)
library(magrittr)
library(dplyr)
library(knitr)
library(readxl)
library(caret)
library(GGally)

```

## Introduction{-}
Clinical visit data was collected from 150 subjects aged between 60 and 96. Data collected included the subjects age, gender, years of education and socioeconomic status (assessed by the Hollingshead Index of Social Position). Each subject underwent MRIs on at least two separate visits (over one year apart) in order to assess estimated intracranial volume and normalised whole brain volume. They were also given a score at each visit for the Mini-Mental State Examination (MMSE), that is used in clinical practice to measure cognitive impairment, and were given a Clinical Dementia Rating. 

The Clinical Dementia Rating (CDR) is a global rating device, first introduced in 1982. It is calculated on the basis of testing six cognitive and behavioural domains such as memory, orientation, judgment, problem solving, community affairs, home and hobbies and personal care. It is based on a five point scale of 0-3; 0 = no dementia, 0.5 = questionable dementia, 1 = mild cognitive impairment/ mild dementia, 2 = moderate cognitive impairment/ moderate dementia and 3 = severe cognitive impairment/ severe dementia [@Morris1997].

The analysis performed here will combine the patient and clinical visit data and tidy this into a format suitable for analysis. Then I aim to determine whether Clinical Dementia Rating can be predicted based on a number factors which are associated with increased risk of dementia such as age and socioeconomic status (**REF**) and factors associated with dementia progression such as intracranial volume (**REF**) and MMSE score. This assessment will be performed using supervised methods of machine learning. I will also assess which machine learning method is able to most accurately predict CDR. 

Analysis was conducted in R [@Rbase] using Rmarkdown [@Allaire2020; @Xie2018; @Xie2020]

## Data Import{-}
```{r data import}
#Read in the patient data and the visit data as two separate .txt files 

patient <- read_excel("data_raw/patient_data.xlsx")

visit <- read_excel("data_raw/visit_data.xlsx")
```

Clinical visit data and patient data were stored in two separate Excel spreadsheets. The two spreadsheets were read into R separately. It was necessary to do this instead of reading them in together as the visit data and patient data had different numbers of observations. There were 150 participants in the study (hence 150 observations for patient data) however each patient had multiple MRI scans on separate visits, so there are 373 observations in the visit data. 

<!-- do I need to show the two data sets str for a visual representation of this? -->
```{r visit and patient structure, include = TRUE}

str(patient)
str(visit)
```

## Data Tidying{-}

The column names in both dataframes are not in a tidy format. **show this here?** Using the `names()` function, the column names can be replaced by tidier names in snake_case. 

```{r data tidy column headings, include = FALSE}

#Change the headings in patient data to a tidy format
#look at existing column names
colnames(patient)

#create and assign new column names
colnames_patient <- c("subject_ID", "group", "sex", "years_of_education", "socioeconomic_status")
names(patient) <- colnames_patient

#Change headings in the visit data to more meaningful names and to a tidy format
#look at existing column names
colnames(visit)

#create and assign new column names
colnames_visit <- c("subject_ID", "MRI_number", "visit", "age", "MMSE_score", "CDR", "intracranial_volume", "whole_brain_volume", "atlas_scaling_factor")
names(visit) <- colnames_visit
```

The visit and patient data were read in as two separate files as they have different amounts of variables. There are a number of different functions which can be used to join two dataframes ("x" and "y") together by a specified join column, which is present in both dataframes. The `inner_join()` function produces a combination dataframe of the two dataframes provided to it based on a specified column. In this case the chosen join column was subject_ID we want to combine all the information **for each visit of each subject** in to one row. The `inner_join()` function creates a dataframe which retains all the rows from dataframes x and y, therefore enables you to join two dataframes with different numbers of observations. If one dataframe (x) has less rows than the other (y) the rows in x will be duplicated based on the observations in the join column. In this case, patient data rows are duplicated for every set of visit data from that patient. Other functions such as `left_join()` and `right_join()` will only keep the rows from either x or y and any extra rows will be deleted. These functions were not used here as it is important to maintain all the visit data.  

```{r innerjoin, include = FALSE}
#Join the patient data and visit data tables together using inner join. this function compared to others enables you to duplicate the patient data rows to joing together based on subject id. You can multiply the patient data so same no obs.
dementia <- inner_join(patient, visit, by = "subject_ID", copy = FALSE, suffix = c(".x", ".y"))
```

The IDs of the subjects in this dataframe are in a format used in the clinic. This format is unnecessary for this analysis and could overcomplicate any summary statistics and visual representations of the data. The subject IDs can be tidied by removing the "OAS2_0" in order to leave a simple three digit number. `str_replace()` is used to replace matched patterns in a string of characters, so can be used to replace "OAS2_0" with nothing (""). Due to the large number of observations, `str_replace()` is first applied a single observation to check whether the function peforms the necessary task correctly. The function can then be applied to all the observations in the column using the `mutate()` function. 

```{r subject IDs, include = FALSE}

#Extract one observation/subject ID to work with 
one_subject_ID <- dementia$subject_ID[1]

one_subject_ID

one_subject_ID %>% 
  str_replace("OAS2_0", "")

#Apply to the whole column
dementia_tidy <- dementia %>% 
  mutate(subject_ID = subject_ID %>% 
           str_replace("OAS2_0", ""))
```


```{r CDR_2, include = FALSE}
#Change 2 for 1
dementia_tidy <- dementia_tidy %>% 
  mutate(CDR_2 = CDR %>% 
           str_replace("2", "1"))
```

From a quick scan of the data upon mport it is clear that there are a number of NA values. NA values within dataframes are detected using the `is.na()` function. The output of this function is a list of every row and a TRUE/FALSE indicator of NA values. Unfortunately this function has a max print limit and due to the large size of this dataset, 297 rows are omited from the output. It is for this reason that the `any(is.na())` and `which(is.na())` functions are used. `any(is.na())` provides a simple output of TRUE or FALSE based on whether or not missing values are present within the dataframe. `which(is.na())` determines what positions in the dataframe the NA values are; the output of which can be seen below. From this, one is able to see that there are 21 missing values within the dataframe. 

```{r NA values, include = TRUE}
which(is.na(dementia_tidy))
```

When the `str()` function is applied to the patient data, it is clear that a number of the NA values are in the socioeconomic status column. It can be determined using `which(is.na(dementia$socioeconomic_status))` that there are 19 NA values within this column. It is unclear where the other NA values are. `na.omit()` is used to remove any rows with missing values. It is important to remove any missing values as they would interfere with the Random Forest and Linear Discriminant Analysis. Other functions exist that can predict what the NA values would be and replace them in the dataframe but this is not possible here as socioeconomic status is an independent variable here and cannot be predicted from the data we have. 

```{r removing NAs, include = FALSE}
#Several subjects do not have a score for socioeconomic status. As I am doing machine learning and want to determine whether we can predict CDR from the different variables I need each variable to have a value for every subject. 
#Hence need to remove NAs

#Determine if there are any NA values
any(is.na(dementia_tidy))
#Output is true - this shows me that somewhere in the dataframe there are NA values. 
which(is.na(dementia_tidy))
#This shows me that there are 21 observations that have an NA value for one of the variables
#From a quick scan through the dataset I suspect that the missing values are in the socioeconomic status column. 
which(is.na(dementia_tidy$socioeconomic_status))
#I can see that 19 of the NAs are in the socioeconomic status column. 
#The other two were found to be in MMSE score
which(is.na(dementia_tidy$MMSE_score))

#We want to remove all the observations which contain an NA value for one or more variables
dementia_no_na <- na.omit(dementia_tidy)

which(is.na(dementia_no_na))
#Proves that there are no more NA values in the dataset


```

## Data summary{-}

<!-- generate some kind of summary stats!! Maybe plots of the data? -->

## Linear Discriminant Analysis{-}

Linear Discriminant Analysis (LDA) uses linear combinations of predictors to predict the class of a given observation. The algorith begins by finding directions that maximise the separation between classes and uses these directions to predict the class of individuals. LDA is performed using the `lda` function from the MASS package (**REF**). This package is not loaded, however as loading it masks the function `select` needed to perform the LDA. Here LDA is peformed with testing and training. The dataset is split into train and test datasets, using the `createDataPartition` function from the caret package (**REF**). The model is built or 'trained' on the train dataset, which is 75% of the original. Usually the aim of a model is to determine whether specific variables have a significant effect on response. This uses all of the data. However, we cannot be sure that the data we have is generalisable and how well that model would predict responses in a new dataset - this is overfitting. Training the model on 75% of the data, and testing it on the remaining 25% enables us to detect the level of overfitting. If the model performs well on the training set but not on the test set it is likely we are overfitting and it is not a good model. 
Clinical dementia rating can take on 4 values - 0, 0.5, 1 and 2 - based on dementia status. In the data used here there are very few scores of 2. Because the test set is only 25% of the original, it has very little chance of containing any 2's. This means the model cannot predict the scores of the test data as the data and its reference (the predictions of the test data) do not have the same levels. It is for this reason that I added a new column to the dementia data. 
In the CDR scale, 0 = no dementia, 0.5 = questionable dementia, 1 = mild dementia and 2 = moderate dementia. Therefore scores of 1 and 2 can be combined to a score of 1, now representing "dementia". The function `mutate()` is used to create a new column ("CDR_2"), which contains the CDR scores. `str_replace()` is used to replace matched patterns in a string of characters, so can be used to replace "2" with nothing "1".

```{r LDA, include = FALSE}
#Create a vector of row numbers to split the dataset into training and testing sets

ids <- createDataPartition(y = dementia_no_na$CDR,
                           p = 0.75, 
                           list = FALSE)

#use dlpyr function to slice rows based on their index and create the two data sets
train <- dementia_no_na %>% slice(ids)
test <- dementia_no_na %>% slice(-ids)

#Train the model
lda <- train %>% 
  select(years_of_education,
         socioeconomic_status,
         age,
         MMSE_score,
         intracranial_volume,
         whole_brain_volume, 
         atlas_scaling_factor) %>% 
  MASS::lda(grouping = train$CDR_2)

#Predict on the training data
plda_train <- train %>% 
  select(years_of_education,
         socioeconomic_status,
         age,
         MMSE_score,
         intracranial_volume,
         whole_brain_volume, 
         atlas_scaling_factor) %>% 
  predict(object = lda)

#examine the confusion matrix...
confusionMatrix(plda_train$class,
                factor(train$CDR_2))

#Accuracy is only 70.7%. Can be 95% certain that this lies between 64.81% and 76.08%. This is significantly better than by predicting the most common class. 

#Predict classes of test data based on LDA model
plda_test <- test %>% 
  select(years_of_education,
         socioeconomic_status,
         age,
         MMSE_score,
         intracranial_volume,
         whole_brain_volume, 
         atlas_scaling_factor) %>% 
  predict(object = lda)

#Examine the confusion matrix
confusionMatrix(plda_test$class,
               factor(test$CDR_2))
#Accuracy 73.86%. 95% confidence that this lies between 63.41% and 82.66%. Still significantly better than predicting the most common class. 

#TO PLOT...
#Extract scores from training set 
lda_labelled_train <- data.frame(plda_train$x,
                                  CDR_2 = train$CDR_2)
#extract the scores from test set
lda_labelled_test <- data.frame(plda_test$x,
                                CDR_2 = test$CDR_2)
```
The model is first trained on the train data set using the `lda()` function. Then the function `predict()` is used to give the CDR predicted by the model for each observation in the train set, based on years of education, socioeconomic status, age, MMSE score, intracranial volume, whole brain volume and atlas scaling factor. The function `confusionMatrix` from the caret package (**REF**) allows us to see the number of correct and  predictions of CDR. It also outputs a measure of accuracy. The model is 70.7% accurate when predicting the CDR of the train data. The P value is less than 0.05, which shows that the model accuracy is significantly better than if it were to just predict the most common class. 
When the model is used to predict the CDRs in the test dataset, it is 73.86% accurate. This is not significantly less than the accuracy for the training set, proving there is little to no ovefitting. Again, the P value is less than 0.05. 

```{r LDA-train, fig.cap="plot of LDA training set"}

source("scripts/theme_stella.R")

lda_labelled_train %>% 
  ggplot(aes(x = LD1, y = LD2, color = factor(CDR_2))) +
  geom_point() +
  theme_stella()

#There is a fair bit of overlap between the different CDRs here. 
```

`ggplot()` can be used to plot LD1 vs LD2 in order to visualise how well the different CDRs separate. When the model is used to predict the classes on the training dataset, the different CDRs do not separate out very well (Figure \@ref(fig:LDA-train)). 

```{r LDA-test, fig.cap="plot of LDA test set"}

lda_labelled_test %>% 
  ggplot(aes(x = LD1, y = LD2, color = factor(CDR_2))) +
  geom_point() +
  theme_stella()

#separates out a bit better. 
```
The CDR predictions based on the test dataset separate out a little better (Figure \@ref(fig:LDA-test)), but there is still a large amount of overlap between 0s and 0.5s. Hence, it is likely that the model would predict scores of 0 and 0.5 incorrectly. 

```{r LDA-both, fig.cap="plot of LDA train and test sets"}

lda_labelled <- rbind(lda_labelled_test, lda_labelled_train)

lda_labelled %>% 
  ggplot(aes(x = LD1, y = LD2, colour = factor(CDR_2))) +
  geom_point() +
  theme_stella()

#not very clear distinction, especially at lower CDRs. 

```
`rbind()` can be used to combine the predictions of the train and test datasets so they can be plotted together (Figure \@ref(fig:LDA-both)). Again, there is a large amount of overlap between the scores of 0 and 0.5. 

## Random Forest{-}
Random Forests is a machine learning method for classification and regression. They operate by constructing a large number of decision trees from different subsets of a training dataset. The decision trees are then used to output the class that is the mode of the classes output by each of the individual trees. Random forest overcomes the common problem of overfitting which is encountered with Decision Tree methods. Random Forest is also well suited to a smaller sample sizes, like the data in this analysis. 
Random Forest is peformed using the randomForest package (**REF**) however, loading this package using the `library` function masks the `margin` function of dplyr (**REF**) and the `margin` function of ggplot2 (**REF**). It is for this reason double colons are used to access functions from randomForest without loading the package itself. 

```{r Random Forest, include = FALSE}
#Set random seed to make results reproducible
set.seed(17)

#calculate the size of each of the datasets
data_set_size <- floor(nrow(dementia_no_na)/2)

#generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(dementia_no_na), size = data_set_size)

#Assign the data to the corrext sets
training <- dementia_no_na[indexes,]
validation1 <- dementia_no_na[-indexes, ]

#Make dependent variable a factor, in order for random forest to do classification not regression
dementia_no_na$CDR <- factor(dementia_no_na$CDR)

#Peform the training
rf <- randomForest::randomForest(formula = CDR~., data = dementia_no_na, ntree = 500, mtry = 3, importance = TRUE)

print(rf)

#find the optimal mtry value with the minimum out of bag(OOB) error

mtry <- randomForest::tuneRF(dementia_no_na[-1], 
               dementia_no_na$CDR,
               ntreeTry = 500,
               stepFactor = 1.5,
               improve = 0.01,
               trace = TRUE, 
               plot = TRUE)
best_m <- mtry[mtry[,2] == min(mtry[,2]), 1]

print(mtry)
print(best_m)

#4 or 6 are the best mtry values

#Use best mtry value
set.seed(17)

rf <- randomForest::randomForest(formula = CDR~., 
                                 data = dementia_no_na, 
                                 ntree = 200,
                                 mtry = best_m, 
                                 importance = TRUE)

print(rf)

#OOB estimate of error rate = 12.43%

#Evaluate the variable importance
randomForest::importance(rf)
randomForest::varImpPlot(rf)

#Higher mean decrease accuracy or mean decrease gini score, the higher the importance of this variable in the model. 
#Here group is highest - need to exclude this. Then MMSE_score is a good predictor of CDR, followed by age, whole brain volume etc etc. Years of education, sex and socioeconomic status have little impact on CDR.  

```
In a Random Forest, the data is first split into imbalanced training and validation datasets.
<!-- There is a hard limit on the number of features which can be considered within a random forest, as when more features are considered, the data becomes sparse which can lead to overfitting. Ideally you would want e^No of features to be less than the number of observations. Here there are 354 observations meaning a maximum of 6 features can be considered without running the risk of overfitting.  -->
Random Forest can be used to investigate either regression or classification. As we are looking to investigate whether CDR can be predicted based on the other factors we want to investigate classification. In Random Forest, classification can only be performed if the dependent variable is a factor/ is categorical. Clinical Dementia Rating here is a numerical value which results in a regression model. Hence it is important to make CDR a factor using the `factor`function.
In a random forest mtry refers to the number of variables used in each tree. The default value for mtry is determined as the square root of the number of predictors in the random forest, which here is 3. However, mtry can take on a range of numbers, and a higher mtry increases both the correlation and strength of the random forest. mtry needs to be optimised to the value which produces the lowest out of bag error rate. Mtry is optimised with the `tuneRF` function which searches for the mtry value with the lowest OOB error estimate.  For this Random Forest, the best values for mtry are 4 or 6 which both have an OOB error rate of 0.847. 
ntree refers to the number of trees to be generated and again, is optimised to the ntree which produces the lowest OOB error estimate. Here the optimal ntree is 200, which produces a OOB estimate of error rate of 11.86%. 
Mean Decrease Accuracy and Mean Decrease Gini show how important a variable is in the model - the higher the score, the more important that variable is. Plots of these can be produced using the `importance` and `varImpPlot` functions of the randomForest package. Here the most important variables, and thus the best predictors of a higher clinical dementia rating, are group and MMSE score, which is expected as they both define dementia status based on assessment. Following this age and whole brain volume have the most importance. It is known that dementia is a condition well associated with ageing and that variations in an individuals whole brain volume can impact on their likelihood of developing dementia (**REF**). 

## Conclusion{-}

**WHY IS THIS NOT WORKING AAAAAA**
```{r wordcount}

wordcount <- wordcountaddin::word_count("dementia.Rmd")
wordcount
#current wordcount is ??
```
**Wordcount:** `r wordcount`

## References{-}
