---
title: "BIO00058M Dementia Case Study"
author: "Examination Number: Y3866839"
date: "30/11/2020"
output: 
  bookdown::html_document2: default
bibliography: references/references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.retina = 3)
```

```{r load-packages, include = FALSE}
library(tidyverse)
library(magrittr)
library(dplyr)
library(knitr)
library(readxl)
library(caret)
library(GGally)
```

## Introduction{-}
Clinical visit data was collected from 150 subjects aged between 60 and 96. Data collected included the subjects age, gender, years of education and socioeconomic status. Each subject underwent MRIs on at least two visits to assess their intracranial volume and whole brain volume. At each visit they were given a score for the Mini-Mental State Examination (MMSE), used in clinical practice to measure cognitive impairment [@Arevalo-Rodriguez2015], and were given a Clinical Dementia Rating. 

The Clinical Dementia Rating (CDR) is a global rating device, calculated on the basis of testing six cognitive and behavioural domains. It is based on a five point scale of 0-3; 0 = no dementia, 0.5 = questionable dementia, 1 = mild dementia, 2 = moderate dementia and 3 = severe dementia [@Morris1997].

The analysis performed here combines patient and visit data and tidies the data into a format suitable for analysis. The analysis uses supervised methods of machine learning to predict CDR based on factors associated with increased risk of dementia, such as age [@Garre-Olmo2018], and factors associated with dementia progression, such as intracranial volume [@Tate2011]. Two methods of machine learning are used here and are compared.

Analysis was conducted in R [@Rbase] using Rmarkdown [@Allaire2020; @Xie2018; @Xie2020]

## Data Import{-}
```{r data-import}
#Read in the patient data and the visit data as two separate .txt files 

patient <- read_excel("data_raw/patient_data.xlsx")

visit <- read_excel("data_raw/visit_data.xlsx")
```

Visit data and patient data were stored in separate Excel spreadsheets. The spreadsheets were read into R separately. There are functions that can directly read in and combine two datasets but the datasets in this investigation have different numbers of observations so needed to be read in separately. There were 150 participants in the study (150 observations in patient data) however each patient had multiple visits and MRI scans, so there are 373 observations in the visit data.

```{r data-structure, include = TRUE}

str(patient)
str(visit)
```

## Data Tidying{-}

The column names in both dataframes are not in a tidy format. Using the `names()` function, the column names can be replaced by tidier names in snake_case. 

```{r column-headings, include = FALSE}

#Change the headings in patient data to a tidy format
#look at existing column names
colnames(patient)

#create and assign new column names
colnames_patient <- c("subject_ID", "group", "sex", "years_of_education", "socioeconomic_status")
names(patient) <- colnames_patient

#Change headings in the visit data to more meaningful names and to a tidy format
#look at existing column names
colnames(visit)

#create and assign new column names
colnames_visit <- c("subject_ID", "MRI_number", "visit", "age", "MMSE_score", "CDR", "intracranial_volume", "whole_brain_volume", "atlas_scaling_factor")
names(visit) <- colnames_visit
```

The visit and patient data were read in as two separate files as they have different amounts of variables. Various 'join' functions combine two dataframes ("x" and "y") via a specified join column, present in both dataframes. In this case the chosen join column was subject_ID. The `inner_join()` function was used here and creates a dataframe which retains all the rows from dataframes x and y, therefore enabling two dataframes with different numbers of rows to be joined. If one dataframe (x) has less rows than the other (y) the rows in x will be duplicated based on the observations in the join column. Here, patient data rows are duplicated for every set of visit data from that patient. 

```{r innerjoin, include = FALSE}
#Join the patient data and visit data tables together using inner join. this function compared to others enables you to duplicate the patient data rows to joing together based on subject id. You can multiply the patient data so same no obs.
dementia <- inner_join(patient, visit, by = "subject_ID", copy = FALSE, suffix = c(".x", ".y"))
```

```{r CDR_2, include = FALSE}
#Change CDR values of 2 for 1s. There are very few scores of 2 and when it comes to splitting the data in LDA this causes issues in which the test set has no 2s and the LDA cannot run. 

dementia <- dementia %>% 
  mutate(CDR_2 = CDR %>% 
           str_replace("2", "1"))
```

There are a number of NA values within the dataframe, that can be detected using the `is.na()` function. The output of this function is a list of every row and a TRUE/FALSE indicator of NA values. This function has a max print limit, meaning 297 rows are omitted from its output. Hence `any(is.na())` and `which(is.na())` are used. `any(is.na())` provides a simple output of TRUE or FALSE based on whether or not missing values are present. `which(is.na())` determines the positions of the NA values; the output of which can be seen below. There are 21 missing values in the dataframe. 

```{r NA-values, include = TRUE}
which(is.na(dementia))
```
The `str()` function is used to give an overview of the dataframe and shows that a lot of the missing values are in the socioeconomic status column. `na.omit()` is used to remove any rows with missing values. It is important to remove missing values as they would interfere with the Random Forest and Linear Discriminant Analysis. Other functions exist that can predict what the NA values would be and replace them in the dataframe. This is not possible here as socioeconomic status is an independent variable and cannot be predicted from the data provided. 

```{r na-omit, include = FALSE}
#Several subjects do not have a score for socioeconomic status. As I am doing machine learning and want to determine whether we can predict CDR from the different variables I need each variable to have a value for every subject. 
#Hence need to remove NAs

#Determine if there are any NA values
any(is.na(dementia))
#Output is true - this shows me that somewhere in the dataframe there are NA values. 
which(is.na(dementia))
#Shows that there are 21 observations that have an NA value for one of the variables
#From a quick scan through the dataset I suspect that the missing values are in the socioeconomic status column. 
which(is.na(dementia$socioeconomic_status))
#19 of the NAs are in the socioeconomic status column. 
#The other two are in MMSE score
which(is.na(dementia$MMSE_score))

#We want to remove all the observations which contain an NA value for one or more variables
dementia_no_na <- na.omit(dementia)

which(is.na(dementia_no_na))
#Proves that there are no more NA values in the dataset

```

## Linear Discriminant Analysis{-}

Linear Discriminant Analysis (LDA) uses linear combinations of predictors to predict the class of a given observation. The algorithm starts by finding directions that maximise the separation between classes and uses these directions to predict the class of individuals. LDA is performed using the `lda` function from the MASS package [@MASS]. This package is not loaded, as doing so masks the `select` function needed to perform the LDA. Here LDA is performed with testing and training. The dataset is split into train and test datasets, using the `createDataPartition` function from the caret package [@caret]. The model is built or 'trained' on the train dataset, which is 75% of the original. Usually a model is used to determine if specific variables have an effect on a response, and is built on all the data present. However, we cannot be sure that the data we have is generalisable and how well the model would predict responses in a new dataset - this is overfitting. Training the model on 75% of the data, and testing it on the remaining 25% enables us to detect the level of overfitting. If the model performs well on the training set but not on the test set it's likely the model is overfitting and is therefore not a good model. 

Clinical dementia rating can take 4 values - 0, 0.5, 1 and 2 - based on dementia status. In our data there are very few scores of 2. Because the test set is only 25% of the original, it is unlikely to contain any 2s. This means the model cannot predict the scores of the test data as the data and its reference (the predictions of the test data) do not have the same levels.
In the CDR scale, 0 = no dementia, 0.5 = questionable dementia, 1 = mild dementia and 2 = moderate dementia. Therefore scores of 1 and 2 can be combined to a score of 1, representing "dementia". The function `mutate()` is used to create a new column ("CDR_2"), which contains the new CDR scores. `str_replace()` is used to replace matched patterns in a string of characters, so can be used to replace "2" with "1".

```{r LDA-data-split, include = FALSE}
#Create a vector of row numbers to split the dataset into training and testing sets

ids <- createDataPartition(y = dementia_no_na$CDR,
                           p = 0.75, 
                           list = FALSE)

#use dlpyr function to slice rows based on their index and create the two data sets
train <- dementia_no_na %>% slice(ids)
test <- dementia_no_na %>% slice(-ids)
```

```{r LDA-training}
#Train the model
lda <- train %>% 
  select(years_of_education,
         socioeconomic_status,
         age,
         MMSE_score,
         intracranial_volume,
         whole_brain_volume, 
         atlas_scaling_factor) %>% 
  MASS::lda(grouping = train$CDR_2)
```


The model is first trained using the `lda()` function. Then the function `predict()` is used to give the CDR scores predicted by the model for each observation in the train set, based on years of education, socioeconomic status, age, MMSE score, intracranial volume, whole brain volume and atlas scaling factor. 

```{r LDA-predict-train}
#Predict on the training data
plda_train <- train %>% 
  select(years_of_education,
         socioeconomic_status,
         age,
         MMSE_score,
         intracranial_volume,
         whole_brain_volume, 
         atlas_scaling_factor) %>% 
  predict(object = lda)


```
 
```{r confusion-matrix-train, include = FALSE}
#examine the confusion matrix...
confusionMatrix(plda_train$class,
                factor(train$CDR_2))

#assign confusion matrix
cm_train <- confusionMatrix(plda_train$class,
                factor(train$CDR_2))

#extract accuracy value for reporting in inline code
accuracy_train <- cm_train$overall['Accuracy']

#Accuracy is only 74.81%. This will vary each time it is run. This is significantly better than by predicting the most common class. 
```
The `confusionMatrix` function from the caret package [@caret] allows us to see the number of correct predictions of CDR and outputs a measure of accuracy. The model accuracy is `r accuracy_train` when predicting the CDR of the train data. The P value is less than 0.05, which shows that the model accuracy is significantly better than if it were to just predict the most common class.  
 
```{r confusion-matrix-train-report, include = TRUE}

confusionMatrix(plda_train$class,
                factor(train$CDR_2))

```
 

```{r LDA-predict-test}
#Predict classes of test data based on LDA model
plda_test <- test %>% 
  select(years_of_education,
         socioeconomic_status,
         age,
         MMSE_score,
         intracranial_volume,
         whole_brain_volume, 
         atlas_scaling_factor) %>% 
  predict(object = lda)

```

```{r confusion-matrix-test, include = FALSE}
#Examine the confusion matrix
confusionMatrix(plda_test$class,
               factor(test$CDR_2))

#assign confusion matrix
cm_test <- confusionMatrix(plda_test$class,
               factor(test$CDR_2))

#extract accuracy value for reporting in inline code
accuracy_test <- cm_test$overall['Accuracy']

#Accuracy 70.45%. This will vary. Still significantly better than predicting the most common class. 

```
When the model is used to predict the CDRs in the test dataset, the accuracy is `r accuracy_test` . This is higher than the accuracy for the train data, suggesting little to no overfitting. Again, the P value is less than 0.05. 

```{r confusion-matrix-test-report, include = TRUE}
confusionMatrix(plda_test$class,
               factor(test$CDR_2))

```


`ggplot()` can be used to plot LD1 vs LD2 in order to visualise how well the different CDRs separate; giving a measure of how well the model is able to predict CDR score. When the model is used to predict the classes on the training dataset, there is a lot of overlap between CDRs. (Figure \@ref(fig:LDA-plot-train)). 

```{r LDA-scores, include = FALSE}
#TO PLOT...
#Extract scores from training set 
lda_labelled_train <- data.frame(plda_train$x,
                                  CDR_2 = train$CDR_2)
#extract the scores from test set
lda_labelled_test <- data.frame(plda_test$x,
                                CDR_2 = test$CDR_2)

```


```{r LDA-plot-train, fig.cap="plot of LDA training set", fig.height = 4, fig.width = 6}

source("scripts/theme_stella.R")

lda_labelled_train %>% 
  ggplot(aes(x = LD1, y = LD2, color = factor(CDR_2))) +
  geom_point() +
  theme_stella()

#There is a fair bit of overlap between the different CDRs here. 
```

The CDR predictions based on the test dataset separate out somewhat better (Figure \@ref(fig:LDA-plot-test)), but there is still a lot of overlap between 0s and 0.5s. Hence, it is likely that the model would predict scores of 0 and 0.5 incorrectly. 

```{r LDA-plot-test, fig.cap="plot of LDA test set", fig.height = 4, fig.width = 6}

lda_labelled_test %>% 
  ggplot(aes(x = LD1, y = LD2, color = factor(CDR_2))) +
  geom_point() +
  theme_stella()

#separates out a bit better. 
```

`rbind()` is used to combine the predictions of the train and test datasets so they can be plotted together (Figure \@ref(fig:LDA-plot-both)). Again, there is a large amount of overlap between the scores of 0 and 0.5. 

```{r LDA-plot-both, fig.cap="plot of LDA train and test sets", fig.height = 4, fig.width = 6}

lda_labelled <- rbind(lda_labelled_test, lda_labelled_train)

lda_labelled %>% 
  ggplot(aes(x = LD1, y = LD2, colour = factor(CDR_2))) +
  geom_point() +
  theme_stella()

#not very clear distinction, especially at lower CDRs. 

```

## Random Forest{-}
Random Forest is a machine learning method for classification and regression. It operates by constructing a large number of decision trees from different subsets of a training dataset. The decision trees are then used to output the class that is the mode of the classes output by each of the individual trees. Random forest overcomes the common problem of overfitting which is encountered with Decision Tree methods and is well suited to a smaller sample sizes, like the data in this analysis. 
Random Forest is peformed using the randomForest package [@randomForest] however, loading this package using the `library()` function masks the `combine()` function of dplyr [@dplyr] and the `margin()` function of ggplot2 [@ggplot2]. It is for this reason double colons are used to access functions from randomForest.

```{r RF-data-split, include = FALSE}
#Set random seed to make results reproducible
set.seed(17)

#calculate the size of each of the datasets
data_set_size <- floor(nrow(dementia_no_na)/2)

#generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(dementia_no_na), size = data_set_size)

#Assign the data to the corrext sets
training <- dementia_no_na[indexes,]
validation1 <- dementia_no_na[-indexes, ]
```

```{r Random-Forest, include = FALSE}
#Make dependent variable a factor, in order for random forest to do classification not regression
dementia_no_na$CDR <- factor(dementia_no_na$CDR)

#Peform the training
rf <- randomForest::randomForest(formula = CDR~., data = dementia_no_na, ntree = 500, mtry = 3, importance = TRUE)

print(rf)
```

```{r optimal-mtry, include = FALSE}
#find the optimal mtry value with the minimum out of bag(OOB) error

mtry <- randomForest::tuneRF(dementia_no_na[-1], 
               dementia_no_na$CDR,
               ntreeTry = 500,
               stepFactor = 1.5,
               improve = 0.01,
               trace = TRUE, 
               plot = TRUE)
best_m <- mtry[mtry[,2] == min(mtry[,2]), 1]

print(mtry)
print(best_m)

#2, 3 or 4 are best mtry values
```

```{r RF-best-mtry, include = FALSE}
#Use best mtry value and best ntree - through trial determined to be 200.
set.seed(17)

rf <- randomForest::randomForest(formula = CDR~., 
                                 data = dementia_no_na, 
                                 ntree = 200,
                                 mtry = best_m, 
                                 importance = TRUE)

print(rf)
#OOB error estimate 3.11%
```

```{r variable-importance-1, include = FALSE}

#Evaluate the variable importance
randomForest::importance(rf)
randomForest::varImpPlot(rf)

#Higher mean decrease accuracy or mean decrease gini score, the higher the importance of this variable in the model. 
#Here group is highest - exclude this. Then MMSE_score is a good predictor of CDR, followed by age, whole brain volume etc etc. Years of education, sex and socioeconomic status have little impact on CDR.  

```

The data is first split into imbalanced training and validation datasets. Random Forest can be used to investigate either regression or classification. As we are looking to investigate whether CDR can be predicted based on the other factors we want to investigate classification. In Random Forest, classification is performed if the dependent variable is a factor/ is categorical. Clinical Dementia Rating here is a numerical value, resulting in a regression, hence it is important to make CDR a factor using the `factor()`function.
In a random forest mtry refers to the number of variables used in each tree. The default value for mtry is determined as the square root of the number of predictors, which here is 3. However, mtry can take on a range of numbers, and a higher mtry increases both the correlation and strength of the random forest. Mtry is optimised with the `tuneRF()` function which searches for the mtry value with the lowest OOB error estimate.  Here the optimal mtry valuess are 2, 3 or 4 which all have an OOB error rate of 0.00847. 
ntree refers to the number of trees to be generated and again, is optimised to the value which gives the lowest OOB error estimate. Here the optimal ntree is 200, which produces a OOB estimate of error rate of 3.11%. 

```{r rf-summary}
print(rf)

```

Mean Decrease Accuracy and Mean Decrease Gini (Figure \@ref(fig:variable-importance-plot)) show how important a variable is in the model - the higher the score, the more important that variable is. Plots of these can be produced using the `importance()` and `varImpPlot()` functions of the randomForest package. Here the most important variables, hence the best predictors of higher clinical dementia rating, are group and MMSE score, which is expected as they both define dementia status. Following this age and whole brain volume have the most importance. This is understandable as dementia is a disease of ageing [@Garre-Olmo2018] and variations in someone's whole brain volume can impact on their likelihood of developing dementia [@Tate2011]. 


```{r variable-importance-plot, fig.cap="Mean Decrease Accuracy and Mean Decrease Gini", fig.height = 5, fig.width = 8}
#Evaluate the variable importance
randomForest::varImpPlot(rf)

```

## Conclusion{-}
Both LDA and Random Forest can be used to predict Clinical Dementia Rating. Linear Discriminant Analysis is able to predict CDR with an accuracy of `r accuracy_test`. This is relatively low which could be due to the relatively small size of this dataset; the model is built on just 266 observations and is tested on 88. The Random Forest method is better suited to smaller sample sizes which would explain the low OOB error rate of just 3.11%. Random Forest also shows which variables are the most important in the model. For these reasons, I believe that Random Forest is a better model in predicting Clinical Dementia Rating.

 <!-- based on factors such as age, socioeconomic status, intracranial volume etc -->
```{r wordcount, include = FALSE}

wc_main <- wordcountaddin::word_count("dementia.Rmd")

wc_readme <- wordcountaddin::word_count("README.md")

wc_session <- wordcountaddin::word_count("sessioninfo.md")

```

#### **Wordcount**{-}

This document: `r wc_main`\
README: `r wc_readme`\

**Total: `r wc_main + wc_readme`**

```{r session-info}
#created session info document. this is linked to in the rmd 
file <- "sessioninfo.md"
writeLines(capture.output(sessionInfo()), file)
```


## References{-}
